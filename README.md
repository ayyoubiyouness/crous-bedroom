# Bouge ETL

11/11/2022

## V1

### Objectif

>Le code pr√©sent dans ce repository consiste en une technologie d'ETL permettant d'extraire des donn√©es de diverses sources (API / Scrapping), de transformer et traiter ces donn√©es puis de les int√©grer dans la base de donn√©es Bouge. L'objectif de ce projet est de construire une infrastructure permettant de r√©guli√®rement mettre √† jour / enrichir le produit Bouge.

### Technologies

- Python `3.9.13`.
- Poetry : librairie python en charge de la gestion des autres librairies python. Installation via `pip install poetry`, puis `poetry install` pour installer le reste des librairies.
- Prefect : librairie python permettant de construire des ETL ([documentation](https://docs.prefect.io/concepts/overview/)). La lecture de la documentation est fortement recommand√©e pour comprendre les diff√©rents concepts. Parmi ces concepts, deux principaux sont √† retenir : la **task**, similaire √† une fonction python, qui correspond √† une √©tape de traitement de l'ETL; le **flow** qui est une entit√© qui englobe les tasks, et qui leur fournit un environnement d'ex√©cution. Pour information, chaque √©tape de l'ETL consistera en un flow, qui eux-m√™mes seront englob√©s dans un flow principal qui d√©finira l'ETL (le point d'entr√©e pour l'ex√©cution du code).

### Architecture de l'ETL

Le code actuel permettant de facilement construire des ETL (Extract Transform Load) √† partir de briques d√©j√† construites. Le code a √©t√© pens√© pour cr√©er un ETL par source √† laquelle on souhaite se connecter. Dans la version **V1** du code, l'unique source disponible est [finishers](https://www.finishers.com/).

Un diagramme d'architecture de l'ETL a √©t√© con√ßu et est disponible [ici](https://www.figma.com/file/lwoGBd1HYb99WXOUFFzlpz/Bouge-ETL?node-id=0%3A1&t=MmPNnOWQm03PXhsB-0).

![Architecture de l'ETL](/assets/architecture_etl.png)

Chaque couleur correspond √† une √©tape de l'ETL : extract (violet), diff√©rents transform (vert) et load (bleu). Pour chaque √©tape, il faudra cr√©er autant de tasks que n√©cessaire, que l'on incluera dans des flows sp√©cifiques. Par exemple pour la source `finishers` on retrouve une task d'extract `task_extract_finishers.py` associ√© √† un flow d'extract `flow_extract_finishers.py`, et ce pour chaque √©tape.

**Points √† retenir :**

- Chaque source de donn√©es aura une √©tape d'**extract sp√©cifique**, _i.e_ il s'agira d'une √©tape manuelle consistant en un script qui se connectera √† une API pour r√©cup√©rer de la donn√©e ou qui r√©alisera du scrapping. Cette √©tape a pour objectif de r√©cup√©rer les donn√©es √† int√©grer en base, filtrer sur les donn√©es que l'on souhaite garder, potentiellement faire une premi√®re mise en forme de la donn√©e (surtout pour le scrapping).

- Chaque source de donn√©es aura une √©tape de **transform sp√©cifique**, _i.e_ √©tape manuelle durant laquelle on passe de donn√©es au format fourni par l'API ou le scrapping √† un format normalis√©. Divers mod√®les de classes python correspondant aux tables de la BDD ont √©t√© d√©velopp√© dans le fichier `src/models/db.py`. Ceci nous permet de passer de donn√©es brutes √† des donn√©es correspondant aux entit√©s propres √† Bouge. (Poi, Address, Activity, etc...). Dans chacune de ces classes python, des m√©thodes de traitement / normalisation ont √©t√© d√©velopp√© pour cr√©er de la donn√©e propre √† partir de donn√©e brute. Par exemple, dans la classe `src/models/db.py/Address` une m√©thode permet de valider que le code insee extrait est bien dans le bon format (string d'une longueur de 5 caract√®res). Ces classes h√©ritent des m√©thodes des classes [Pydantic](#https://pydantic-docs.helpmanual.io/) (librairie python similaire aux _dataclasses_ permettant de normaliser facilement des instances d'une classe Python, cf la documentation). √Ä noter que dans le fichier `src/models/db.py` pour chaque classe il y a une version "pydantic" de la classe (par exemple `Address`) qui impl√©mente les m√©thodes de normalisation que l'on utilise dans les √©tapes de transform, et une version "sqlalchemy" (par exemple `AddressSQL`) qui correspond aux objects que l'on poussera en base en utilisant l'ORM SQLAlchemy. Enfin, il est important de renvoyer en sortie des flows de transform sp√©cifiques une liste de mod√®les d√©finis dans `src/models/db.py`. Par exemple, pour `finishers`, la sortie de `flow_transform_finishers.py` renvoie une liste de mod√®les `[Poi(...), Address(...), Activity(...), AddressPoi(...), ...]`. Ceci est obligatoire et attendu en entr√©e du transform g√©n√©rique dont parle plus bas.

- Une √©tape de **transform g√©n√©rique** : chaque ETL partagera la m√™me √©tape de transform g√©n√©rique, qui, √† partir des mod√®les extraits par les transforms sp√©cifiques pr√©c√©dents, appliquera diff√©rentes transformations propre aux mod√®les dans `src/models/db.py`. En r√©sum√©, l'√©tape de transform sp√©cifique consiste √† passer de donn√©es brutes √† des donn√©es sous formes de mod√®les Python (correspondant aux entit√©s de la DB Bouge.), puis l'√©tape de transform g√©n√©rique se charge d'appliquer les transformations relatives √† ces mod√®les Python. Par exemple, pour les objets de la classe `Address` on retrouve une m√©thode de transform qui r√©cup√®re l'addresse exacte √† partir de coordonn√©es GPS (reverse geocoding). ‚ö†Ô∏è Point important, les m√©thodes de transform g√©n√©rique au sein des mod√®les doivent √™tre pr√©fix√©es par `transform_`, _i.e_ dans la classe `Address` on trouve la m√©thode `transform_get_address()`.

- Une √©tape de **load g√©n√©rique** : chaque ETL partagera la m√™me √©tape de load g√©n√©rique, qui se charge de transformer les objets des classes "pydantic" (par exemple Address), en objets des classes "sqlalchemy" (par exemple AddressSQL). Une fois cette conversion faite, les objets sont pouss√©s en base de donn√©es.


_**NB: Pour le moment, les tasks et flows g√©n√©riques sont fonctionnels et ne doivent pas subir de modifications. Ces scripts peuvent √™tre r√©-utilis√©s tels quels pour chaque nouvel ETL.**_

_**NB: En fonction des sources de donn√©es, il faudra potentiellement cr√©er de nouveaux mod√®les permettant de mod√©liser les entit√©s de la source en objets python et d'y int√©grer des m√©thodes de normalisation. Par exemple, pour `finishers`, deux classes (Event et Race) ont √©t√© cr√©√© dans `src/models/generic.py` pour mod√©liser les √©v√©nements et les courses finishers. Pour chaque nouvelle source, il peut √™tre int√©ressant de venir cr√©er de nouveaux mod√®les dans ce fichier pour faciliter le traitement de la donn√©e en Python.**_

Repr√©sentation de l'ETL Finishers üëáüèª

![D√©tails de l'√©tape de transform](/assets/details_transform.png)

#### D√©tails techniques

G√©n√©ralement pour chaque nouvel ETL, l'ordre d'ex√©cution des flows est le suivant :

1) √âtape d'extract r√©alis√©e de mani√®re s√©quentielle (on attend que toutes les donn√©es soient extraites avant de passer √† la suite).
2) √âtape de transform sp√©cifiques r√©alis√©e de mani√®re concurrentielle / parall√®le. Chaque entit√© que l'on traite peut √™tre trait√©e ind√©pendamment des autres. Par exemple, pour `finishers`, dans l'√©tape d'extract on extrait des √©v√©nements / Event (chaque √©v√©nement contient au moins une course / Race). On ex√©cute un flow de transform sp√©cifique (`flow_transform_finishers`) par √©v√©nement extrait, et ce de mani√®re parall√®le car ils sont ind√©pendants entre eux.
3) √âtape de transform g√©n√©rique, qui prend en entr√©e tous les mod√®les g√©n√©r√©s par les transforms sp√©cifiques, et qui est r√©alis√© de mani√®re s√©quentielle (_i.e_ on traite chaque entit√© l'un apr√®s l'autre). Ce n'est pas optimis√© pour le moment, on pourrait le faire de mani√®re concurrente.
4) √âtape de load, ex√©cut√©e de mani√®re s√©quentielle. Sur ce point, pour des raisons de partage de ressources (DB), on doit r√©aliser cette √©tape de mani√®re s√©quentielle (car la DB a √©t√© mal configur√©e, les ids des lignes que l'on cr√©√© en base sont g√©n√©r√©s par la DB et non par les valeurs qu'on lui envoie...). C.f [identity](https://www.postgresqltutorial.com/postgresql-tutorial/postgresql-identity-column/). 

### Architecture du code

#### Flows & Tasks

‚û™ Les **tasks** sont stock√©es dans le dossier `src/etl/tasks`. Au sein de ce dossier, on retrouve un dossier `generic` qui contiendra l'ensemble des tasks g√©n√©riques et communes √† tous les ETLs. Ensuite, on retrouvera autant de dossiers que des sources existantes. Pour le moment, dans la V1, un seul autre dossier est pr√©sent est s'appelle `finishers`. Chacun de ces dossiers sp√©cifiques contiendra les tasks sp√©cifiques √† la source en question.

‚û™ Les **flows** sont stock√©s dans le dossier `src/etl/flows`. M√™me structure que pour les tasks mais au niveau des flows. A priori, seuls des flows de transforms sp√©cifiques sont √† ajouter dans de nouveaux dossiers sp√©cifiques aux nouvelles sources.


#### Models

‚û™ Les mod√®les / classes python correspondant √† des entit√©s de la DB Bouge. sont stock√©s dans le fichier `src/models/db.py`.
‚û™ Les mod√®les / classes python g√©n√©riques (_i.e_ non li√©s √† Bouge, autrement dit, li√©s √† des sources de donn√©es externes) sont stock√©s dans le fichier `src/models/generic.py`. Par exemple, c'est ici que l'on retrouve les classes **Race** et **Event** correspondant aux courses et √©v√©nements de  `finishers`.

#### Services

‚û™ Les services / m√©thodes python propres √† chacune des sources sont stock√©s dans des fichiers sp√©cifiques au nom de celles-ci. Par exemple, pour `finishers`, on retrouve le fichier `src/services/finishers.py`, qui g√®re toute la logique de traitement de donn√©es propre √† cette source. C'est au sein de ce fichier que la plus part du code doit se trouver, de sorte √† ce que les tasks et flows soient tr√®s concis et simples √† lire.

‚û™ Les services / m√©thodes python g√©n√©riques (_i.e_ non li√©s √† Bouge, autrement dit, li√©s √† des sources de donn√©es externes) sont stock√©s dans le fichier `src/services/generic.py`. Par exemple, c'est ici que l'on retrouve des m√©thodes utiles √† tout le projet. Ce fichier sera potentiellement rapatri√© dans `src/utils/` par la suite.

#### Utils

‚û™ Le dossier `src/utils` est un dossier qui contiendra diff√©rents scripts utiles √† tout le projet, et qui ne trouvent pas leur place ailleurs dans le projet. On y retrouve notamment un fichier `db.py` qui s'occupe de toutes les diff√©rentes int√©ractions / requ√™tes √† la DB PostgreSQL.

#### Tests

‚û™ Le dossier `tests` contient tous les scripts de tests. On y teste de mani√®re unitaire toutes les fonctionnalit√©s que l'on a d√©velopp√© dans les √©tapes pr√©c√©demment mentionn√©es. Pour fonctionner, ces scripts doivent √™tre pr√©fix√©s par `test_` de la m√™me mani√®re que les m√©thodes qu'il contient. C'est la librairie `pytest` qui se charge d'ex√©cuter ces tests (√† chaque push sur Github). Par la suite, une commande `test` sera ajout√©e au fichier Makefile, pour pouvoir lancer les tests avec la commande `make test`.

#### Deploiement

‚û™ **VPS** : l'ensemble du projet est d√©ploy√© sur un serveur virtuel (_VPS B1ms ~17$/mois_) Azure Cloud.
‚û™ **Dockerfile** : une image docker, copi√©e d'une image Prefect, a √©t√© d√©velopp√©e pour d√©finir les diff√©rents containers.
‚û™ **Docker Compose** : une fichier `docker-compose.yml` a √©t√© d√©velopp√© pour cr√©er les diff√©rents containers responsables de faire tourner l'application (_c.f plus bas dans la section **Back**_).
‚û™ **Makefile** : une fichier Makefile est disponible pour ex√©cuter 2 diff√©rentes commandes : `make docker` pour build l'image docker (n√©cessaire d√®s qu'un changement est apport√© au sein du dossier `src/`, `make register-finishers-flow` pour cr√©er un d√©ploiement d'un ETL accessible sur le front (c.f le concept de d√©ploiement dans la documentation de Prefect).
‚û™ **prefect.sh** : script bash en charge d'instancier les diff√©rents containers d√©finis dans le `docker-compose.yml`.

### Produits

#### Front

‚û™ L'interface front de Prefect est accessible √† l'addresse suivante : http://51.11.240.139:4200/runs. On peut y monitorer les diff√©rentes ex√©cutions d'ETL, voir les logs, modifier la programmation d'ex√©cution d'un ETL, voir les diff√©rents ETL enregistr√©s ou encore lancer manuellement l'ex√©cution d'un ETL.

#### Back

Globalement le code pr√©sent sur Github est h√©berg√© sur un VPS Azure Cloud. C'est ce serveur qui va ex√©cuter le code au sein d'un Docker Container d√©fini dans le fichier `docker-compose.yml`. 

Dans ce docker-compose sont d√©finis 3 containers : `postgres` pour la DB PostgreSQL requise par Prefect pour stocker les logs des ex√©cutions d'ETL (toutes les donn√©es qui sont visibles sur le front sont stock√©es dans cette base de donn√©es locale), `prefect-server` qui g√®re l'application front, et `prefect-agent` qui d√©finit un processus qui attend que l'on demande d'ex√©cuter un ETL (il reste toujours √† l'√©coute de lancements, envoy√©s par le front via un call API).

### Utilisation du code

Voici les principales commandes √† ex√©cuter pour mettre le service en place sur le serveur.

#### Tests en local

‚ö†Ô∏è Il est important de tester le code en local avant tout push sur Github. Pour se faire, il faut ouvrir 3 fen√™tres de terminal :

**Terminal 1 | Lancement du front**  
Permet d'acc√©der √† l'interface UI de Prefect, accessible sur `http://localhost:4200/`

‚û™ `prefect server start`

**Terminal 2 | Lancement du back**  
D√©finit un agent qui va √©couter les demandes d'ex√©cution du code. C'est sur ce terminal que seront ex√©cut√©s les ETL, et donc c'est ici que l'on retrouvera les logs des ex√©cutions.

‚û™ `prefect agent start -q daily`

**Terminal 3 | Lancement des runs (optionnel)**  
Terminal qui permet de lancer des runs via manuellement. L'autre option est de lancer les runs depuis l'UI.

‚û™ `prefect deployment build src/etl/flows/<source>/flow_etl_<source>.py:etl -n dev -t <source> -ib process/process-dev -q daily --timezone 'Europe/Paris' # Permet de d√©finir un nouvel ETL, √† ex√©cuter √† chaque changement du code`

‚û™ `prefect deployment apply etl-deployment.yaml # Permet d'enregistrer le nouvel ETL sur l'UI, et donc de le rendre accessible via l'UI`

‚û™ `prefect deployment run etl-<source>/dev # Demande une ex√©cution de l'ETL`

‚ö†Ô∏è _Important : veiller √† remplacer `<source>` par le nom de la source li√©e √† l'ETL (par exemple `finishers`)._


#### D√©ploiement
‚û™ R√©cup√©ration du code sur Github (_actuellement le code est host√©e sur mon espace JeanSavary, il devra √™tre host√©e dans l'environnement de travail de Bouge._)

```bash
cd bouge-etl

git pull origin main # retrieve code from github
```

‚û™ Lancement du service

```bash
make docker # build the Docker image

bash prefect.sh start # execute the .sh file, build 3 containers

make register-finishers-flow # register the flow related to finishers
```
## TODO

### Good practices

‚û™ Pour tout ajout d'une nouvelle source, il faut :
1) Cr√©er une nouvelle branche : `git checkout -b etl-<source>` (remplacer `<source>` par le nom de la source, ajouter des underscores si la source est compos√©e de plusieurs mots).
2) D√©velopper le code n√©cessaire, faire les commits et les push sur cette nouvelle branche.
3) Tester le code via les commandes mentionn√©es plus haut.
4) Faire une pull-request sur la branche `dev` depuis la nouvelle branche.
5) Ajouter un collaborateur comme **assignee** de la pull-request pour une review du code. Une fois la review termin√©e, le code sera merg√© sur `dev` et puis sur `main` une fois l'ensemble des tests fonctionnels r√©alis√©s.

üí™üèº Bon courage !

### Updates

 - [x] Add row to RegisteredPoi when an event is uploaded to DB
 - [ ] Add name / title to Activity table
 - [ ] Add workflow that publish to ECR after passing tests
 - [ ] If an event has been updated (_i.e sha256 different_), don't duplicate the POI, update it based on the `orignal_id` (+ keep track of changes ?)
 - [ ] Add missing tests
 - [ ] Add new ETL from already developped scrapping scripts

### Additions

- [ ] Add a new source from an existing scrapping script.
